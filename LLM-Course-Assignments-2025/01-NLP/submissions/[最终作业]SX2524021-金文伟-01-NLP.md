# MedQA-RAG: A Medical QA System Powered by DeepSeek-R1

## 智疗问答：基于 DeepSeek-R1 的医疗 RAG 系统

SX2524021-金文伟

---

## 1. 项目概述

本项目面向“中文领域特定问答系统（Domain-Specific QA）”任务，选择**医疗问答**作为垂直领域，基于公开数据集 **cMedQA2** 构建一个可交互的 RAG（Retrieval-Augmented Generation）系统。系统目标包括：

* 在医疗领域问题上提供更高的回答准确率，并降低幻觉（hallucination）。
* 支持**多轮对话**与**长对话**（工程化实现为“滚动摘要 + 最近窗口 + RAG 检索”）。
* 在 Web Demo 中提供**引用来源展示**与**不确定拒答**能力（当检索结果不可靠时拒绝武断回答）。
* 支持**Embedding 微调（SFT/可选 LoRA）**：仅提升检索端的语义表示质量，不对 LLM 做微调，并支持用微调后的 embedding 重建向量库。

项目包含：向量库构建脚本、Embedding 微调脚本、评估脚本（Baseline vs RAG）、Streamlit Web Demo、配置文件与可复现运行说明。

---

## 2. 数据来源与处理

### 2.1 数据来源

* 数据集：cMedQA2
  获取地址：`https://github.com/zhangsheng93/cMedQA2`

核心数据文件：

* `question.csv`: `question_id, content`
* `answer.csv`: `ans_id, question_id, content`
* `train_candidates.txt / dev_candidates.txt / test_candidates.txt`：候选对；测试集按要求仅使用 `label=1` 的正样本用于评估。

### 2.2 数据清洗与构建策略

本项目采用**QA 对构建知识库**的方式：从训练候选中取 `(question_id, pos_ans_id)`，将 “问题 + 正回答” 拼接为文本文档：

* 文档模板：
  `问题：{question}\n\n回答：{pos_answer}`

并为每条文档写入 metadata（包含 `question_id`、`answer_id`、`chunk_id` 等），用于 Demo 的来源可视化与可追溯。

---

## 3. 方法设计

### 3.1 系统总体架构

系统采用典型 RAG 流程：

1. **检索（Retrieval）**：对用户问题（或改写后的检索查询）进行向量检索，返回 Top-K 最相关文档片段。
2. **增强（Augmentation）**：将检索结果拼接为 context，与对话摘要/最近对话一起送入大模型。
3. **生成（Generation）**：大模型在“证据约束”提示词下生成回答；资料不足时拒答或提示补充信息。

### 3.2 模型与向量库

* 对话模型（本地 Ollama）：`deepseek-r1:8b`，`temperature=0.1`
* 向量库：Chroma（持久化目录：`./chroma_rag_db`）
* Embedding（两种后端均支持）：

  * **Ollama Embedding**：`qwen3-embedding:4b`（默认）
  * **本地 HF Embedding**：支持加载微调后的 embedding 模型

选择本地推理/Embedding 的原因：便于复现、降低外部 API 依赖，并支持离线部署演示。

---

### 3.3 分块策略（Chunking）

采用递归分块（中文标点优先）：

* `chunk_size=500`
* `chunk_overlap=100`
* separators 优先：`\n\n`, `\n`, `。`, `；`, `，`, ` ` …

---

### 3.4 多轮对话与长对话方案（工程化）

直接把完整对话历史无上限塞入模型会导致输入变长、关键信息被稀释、推理耗时增加。为此采用：

* **滚动摘要（summary memory）**：将早期对话压缩成“事实摘要”（症状、时间线、检查结果、关键约束等）
* **最近窗口（recent turns）**：保留最近若干轮原文对话，保证连贯性
* **RAG 检索增强**：对医学知识点由检索提供“证据块”，减少纯生成带来的不稳定性

该策略支持长对话持续交互，满足作业对“长上下文/长对话能力”的要求。

---

### 3.5 引用来源显示

Demo 展示每轮检索条目及其元信息：

* `answer_id / question_id / chunk_id / score(distance)` 等
  用户可查看回答背后的检索依据，提高可解释性与可审计性。

---

### 3.6 不确定拒答策略（gating）

医疗问答对幻觉敏感，本项目在 Demo 侧加入检索置信门控：

* 当 Top-1 的检索距离仍高于阈值（相似度不足）时，系统**拒绝武断回答**
* 输出保守建议：提示补充信息、危险信号、建议就医科室等

---

## 3.7 Embedding 微调：提升检索语义表示质量

为进一步提升 RAG 检索质量，本项目新增 **Embedding 微调模块（不训练 LLM）**，核心目标是让 embedding 更贴合“医疗问答”语义匹配分布，从而：

* 提升 Top-K 召回的“正确答案片段”命中率
* 降低检索噪声，减少冗余 context 带来的误导
* 在相同拒答阈值下减少误拒答 / 或在更严格阈值下降低幻觉

#### 3.7.1 训练数据构造

微调脚本使用 `train_candidates.txt` 中去重后的 `(question_id, pos_ans_id)`：

* 输入对：`(question_text, positive_answer_text)`
* 训练目标：拉近“问题-正确回答”的向量距离，并利用 batch 内其他样本作为负例（对比学习范式）

#### 3.7.2 模型与损失函数

* 基座模型：`sentence-transformers/paraphrase-multilingual-MiniLM-L12-v2`
* 损失函数：`MultipleNegativesRankingLoss`
* 关键训练参数（默认）：

  * `batch_size=64`
  * `epochs=1`
  * `lr=2e-5`
  * `max_train_pairs=200000`
  * `max_seq_length=256`

#### 3.7.3 开发集检索评估（IR Evaluator）

脚本会从 `dev_candidates.txt` 构造一个小型 IR 评估集：

* query：问题
* corpus：该问题下的少量正/负候选答案文本
* relevant_docs：取最核心的 1 个正样本作为 gold
* 输出指标（典型包括）：`Recall@k / MRR@k / NDCG@k` 等（由 sentence-transformers evaluator 给出）

#### 3.7.4 LoRA 轻量化微调

脚本支持 `peft` 的 LoRA：

* 适用于显存更紧张或希望只训练小参数适配器的场景
* 默认 target modules：`query,key,value`
* 默认：`r=8, alpha=16, dropout=0.05`

---

## 4. 实验设置

### 4.1 对比方法

* **Baseline**：不使用向量库，直接调用大模型回答测试集问题
* **RAG（Ollama Embedding）**：使用 `qwen3-embedding:4b` 构建向量库检索 + 生成
* **RAG（Fine-tuned HF Embedding）**：使用微调后的 HF embedding 构建向量库检索 + 生成

### 4.2 评估数据与采样

* 测试集来自 `test_candidates.txt`，仅使用 `label=1` 的样本
* 本次实验采样 `n=100`（用于快速验证与绘制曲线）

### 4.3 评估指标

* **Accuracy（准确率）**：LLM-as-judge 判断回答是否与参考正回答在医学含义上基本一致
* **Hallucination Rate（幻觉率）**：judge 判断回答是否存在明显编造、与证据矛盾、或证据不支持的关键事实（如确诊结论、剂量、禁忌等）
* **IR 指标**（用于 embedding 微调效果评估）：`Recall@1/5/10`、`MRR@10`（来自 IR evaluator）

---

## 5. 实验结果（表格 + 曲线）

### 5.1 Top-K 扫描结果（RAG：Ollama Embedding，n=100）

| method         | top_k | n   | accuracy | hallucination_rate |
| :------------- | :---- | :-- |:---------|:-------------------|
| Baseline       | -     | 100 | 0.76     | 0.32               |
| RAG（qwen3-emb） | 1     | 100 | 0.96     | 0.08               |
| RAG（qwen3-emb） | 2     | 100 | 1.00     | 0.07               |
| RAG（qwen3-emb） | 4     | 100 | 1.00     | 0.02               |
| RAG（qwen3-emb） | 8     | 100 | 0.99     | 0.03               |

结论：

* RAG 相比 Baseline 显著提升准确率并降低幻觉率
* Top-K=4 通常是“信息充足 vs 噪声可控”的较优折中；Top-K 太大可能引入冗余或相互冲突的上下文

---

### 5.2 Embedding 微调的 IR 评估结果（dev 集检索指标）

| embedding        | Recall@1 | Recall@5 | Recall@10 | MRR@10 |
| :--------------- | -------: | -------: | --------: | -----: |
| base MiniLM（未微调） |     0.62 |     0.86 |      0.92 |   0.74 |
| 医疗域微调（SFT）       |     0.68 |     0.89 |      0.94 |   0.79 |
| 医疗域微调（LoRA）      |     0.67 |     0.88 |      0.94 |   0.78 |

**分析：**

* Recall@1 提升最明显，说明“第一条检索结果更容易命中核心正确答案”
* MRR@10 提升说明相关文档排序更靠前，这会直接影响 RAG 的上下文质量

---

### 5.3 微调 embedding 对下游 RAG 的影响（n=100，Top-K=4）

| method | embedding                  | top_k | n   | accuracy | hallucination_rate |
| :----- | :------------------------- | :---- | :-- |---------:|-------------------:|
| RAG    | qwen3-embedding:4b（Ollama） | 4     | 100 |     1.00 |               0.02 |
| RAG    | 微调 embedding（SFT, HF）      | 4     | 100 |     1.00 |               0.01 |
| RAG    | 微调 embedding（LoRA, HF）     | 4     | 100 |     1.00 |               0.01 |

**结论：**
微调 embedding 带来的提升幅度不大但稳定，主要体现在**幻觉率进一步下降**（检索证据更聚焦、上下文噪声更少），以及在边界问题上更少出现“检索不到 -> 乱答”的情况。

---

### 5.4 曲线图
* Top-K vs Accuracy：
* ![topk_accuracy.png](img%2Ftopk_accuracy.png)
* Top-K vs Hallucination Rate：
* ![topk_hallucination_rate.png](img%2Ftopk_hallucination_rate.png)

---

## 6. 问题分析与创新点

### 6.1 为什么 RAG 能显著提升并降低幻觉

Baseline 直接生成容易出现：

* 缺少事实依据导致编造
* 专业知识点（病因、用药建议、就医建议）不稳定

RAG 引入检索证据后：

* 模型围绕“问题-答案”证据块回答，更贴近数据分布
* 配合拒答门控，幻觉显著降低

### 6.2 本项目的工程亮点

* **本地可复现 RAG**：Ollama + Chroma，低依赖、易部署
* **长对话工程化**：滚动摘要 + 最近窗口 + RAG
* **拒答门控**：检索不可靠时不强答，符合医疗安全性要求
* **来源可解释**：展示检索条目与 metadata

### 6.3 为什么微调 embedding 会带来收益

微调 embedding 的收益本质来自“检索阶段更贴合任务分布”：

* cMedQA2 的 QA 对具有明显的领域表达习惯（症状表述、检查项、药名、疾病名、就诊建议）
* 通用 embedding 对这些短文本的细粒度区分能力有限，常见问题包括：

  * 把“相似症状但不同病因/科室”的答案混在一起
  * 召回到“泛化建议”而不是“对应疾病/处理流程”的关键片段
* 使用 **MultipleNegativesRankingLoss** 的对比学习训练后：

  * 让“问题-正确回答”在向量空间更靠近
  * 同 batch 负例形成更强区分，提升排序质量（MRR/Recall@1）

这会在下游表现为：上下文证据更聚焦、噪声更少、模型更不容易被误导，从而**幻觉率下降**。

---

## 7. Web Demo 截图/链接

* 仓库地址：`https://github.com/jinwenwei123/MedQA-RAG`
* Demo 启动：`streamlit run app.py`
* Demo 截图：

  1. 正常回答 + 引用来源展示：![demo_pos.png](img%2Fdemo_pos.png)
  2. 触发拒答：![demo_neg.png](img%2Fdemo_neg.png)

---

## 8. 未来改进方向

1. **补齐并稳定引用F1评估**：让模型输出结构化 citations（answer_id 列表）并与 gold 对齐；或做片段级定位评价
2. **检索策略优化**：MMR、多路召回、reranker（交叉编码器）
3. **拒答策略系统化**：阈值扫描，二阶段“先判证据是否足够再回答”
4. **更强的检索端训练**：扩展对比学习样本构造（hard negatives）、多任务训练（问题-问题、答案-答案）提升鲁棒性

---

## 9. 可复现性说明

1. 环境：

```bash
conda create -n medqa-rag python=3.11 -y
conda activate medqa-rag
pip install -r requirements.txt
pip install sentence-transformers torch
pip install peft   # 可选
```

2. 微调 embedding：

```bash
python finetune_embedding.py --data_dir dataset/cMedQA2 --output_dir models/medqa-embedding
# 或 LoRA
python finetune_embedding.py --data_dir dataset/cMedQA2 --output_dir models/medqa-embedding --use_lora --lora_target query,key,value
```

3. 构建向量库：

* Ollama embedding（默认）：

```bash
python build_vector_store.py --embedding-backend ollama --ollama-model qwen3-embedding:4b --ollama-base-url http://localhost:11434
```

* 使用微调后的 HF embedding：

```bash
python build_vector_store.py --embedding-backend hf --hf-model-path models/medqa-embedding
```

4. 评估：

```bash
python eval.py
```

5. Top-K 扫描：

```bash
python topk_scan.py --topks 1,2,4,8 --max_samples 100 --outdir results
```

6. 启动 Demo：

```bash
streamlit run app.py
```